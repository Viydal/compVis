{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lukRIviHRw-c",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "# Computer Vision 2023 Assignment 2: Image matching and retrieval\n",
        "\n",
        "In this prac, you will experiment with image feature detectors, descriptors and matching. There are 3 main parts to the prac:\n",
        "\n",
        "- matching an object in a pair of images\n",
        "- searching for an object in a collection of images\n",
        "- analysis and discussion of results\n",
        "\n",
        "## General instructions\n",
        "\n",
        "As before, you will use this notebook to run your code and display your results and analysis. Again we will mark a PDF conversion of your notebook, referring to your code if necessary, so you should ensure your code output is formatted neatly.\n",
        "\n",
        "***When converting to PDF, include the outputs and analysis only, not your code.*** You can do this from the command line using the `nbconvert` command (installed as part of Jupyter) as follows:\n",
        "\n",
        "    jupyter nbconvert Assignment2.ipynb --to pdf --no-input --TagRemovePreprocessor.remove_cell_tags 'remove-cell'\n",
        "\n",
        "This will also remove the preamble text from each question. It has been packaged into a small notebook you can run in colab, called notebooktopdf.ipynb\n",
        "\n",
        "\n",
        "We will use the `OpenCV` library to complete the prac. It has several built in functions that will be useful. You are expected to consult documentation and use them appropriately.\n",
        "\n",
        "As with the last assignment it is somewhat up to you how you answer each question. Ensure that the outputs and report are clear and easy to read so that the markers can rapidly assess what you have done, why, and how deep is your understanding. This includes:\n",
        "\n",
        "- sizing, arranging and captioning image outputs appropriately\n",
        "- explaining what you have done clearly and concisely\n",
        "- clearly separating answers to each question\n",
        "\n",
        "## Data\n",
        "\n",
        "We have provided some example images for this assignment, available through a link on the MyUni assignment page. The images are organised by subject matter, with one folder containing images of book covers, one of museum exhibits, and another of urban landmarks. You should copy these data into a directory A2_smvs, keeping the directory structure the same as in the zip file.  \n",
        "\n",
        "Within each category (within each folder), there is a “Reference” folder containing a clean image of each object and a “Query” folder containing images taken on a mobile device. Within each category, images with the same name contain the same object (so 001.jpg in the Reference folder contains the same book as 001.jpg in the Query folder).\n",
        "The data is a subset of the Stanford Mobile Visual Search Dataset which is available at\n",
        "\n",
        "<http://web.cs.wpi.edu/~claypool/mmsys-dataset/2011/stanford/index.html>.\n",
        "\n",
        "The full data set contains more image categories and more query images of the objects we have provided, which may be useful for your testing!\n",
        "\n",
        "Do not submit your own copy of the data or rename any files or folders! For marking, we will assume the datasets are available in subfolders of the working directory using the same folder names provided.\n",
        "\n",
        "Here is some general setup code, which you can edit to suit your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp_2sM9OU-nO"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Badc-TlFVRRg"
      },
      "outputs": [],
      "source": [
        "# !pwd\n",
        "# %cd drive/MyDrive/Colab\\ Notebooks\n",
        "# !pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KESkyVgdRw-g",
        "outputId": "9af4919d-e31f-457d-e8da-ef9fb52eb74e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# Numpy is the main package for scientific computing with Python.\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Matplotlib is a useful plotting library for python\n",
        "import matplotlib.pyplot as plt\n",
        "# This code is to make matplotlib figures appear inline in the\n",
        "# notebook rather than in a new window.\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots, can be changed\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# Some more magic so that the notebook will reload external python modules;\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%reload_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cm6vrr1eRw-j"
      },
      "outputs": [],
      "source": [
        "def draw_outline(ref, query, model):\n",
        "    \"\"\"\n",
        "        Draw outline of reference image in the query image.\n",
        "        This is just an example to show the steps involved.\n",
        "        You can modify to suit your needs.\n",
        "        Inputs:\n",
        "            ref: reference image\n",
        "            query: query image\n",
        "            model: estimated transformation from query to reference image\n",
        "    \"\"\"\n",
        "    h,w = ref.shape[:2]\n",
        "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
        "    dst = cv2.perspectiveTransform(pts,model)\n",
        "\n",
        "    img = query.copy()\n",
        "    img = cv2.polylines(img,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
        "    plt.imshow(img, 'gray'), plt.show()\n",
        "\n",
        "def draw_inliers(img1, img2, kp1, kp2, matches, matchesMask):\n",
        "    \"\"\"\n",
        "        Draw inlier between images\n",
        "        img1 / img2: reference/query  img\n",
        "        kp1 / kp2: their keypoints\n",
        "        matches : list of (good) matches after ratio test\n",
        "        matchesMask: Inlier mask returned in cv2.findHomography()\n",
        "    \"\"\"\n",
        "    matchesMask = matchesMask.ravel().tolist()\n",
        "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
        "                    singlePointColor = None,\n",
        "                    matchesMask = matchesMask, # draw only inliers\n",
        "                    flags = 2)\n",
        "    img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches,None,**draw_params)\n",
        "    plt.imshow(img3, 'gray'),plt.show()\n",
        "\n",
        "def drawlines(img1,img2,lines,pts1,pts2):\n",
        "    ''' img1 - image on which we draw the epipolar lines\n",
        "        img2 - image where the points are defined for visualizing epilines in image 1\n",
        "        pts1, pts2 are your good matches in image 1 and 2.\n",
        "        lines - corresponding epilines '''\n",
        "    r,c = img1.shape\n",
        "    img1 = cv.cvtColor(img1,cv.COLOR_GRAY2BGR)\n",
        "    img2 = cv.cvtColor(img2,cv.COLOR_GRAY2BGR)\n",
        "    for r,pt1,pt2 in zip(lines,pts1,pts2):\n",
        "        color = tuple(np.random.randint(0,255,3).tolist())\n",
        "        x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
        "        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
        "        img1 = cv.line(img1, (x0,y0), (x1,y1), color,1)\n",
        "        img1 = cv.circle(img1,tuple(pt1),5,color,-1)\n",
        "        img2 = cv.circle(img2,tuple(pt2),5,color,-1)\n",
        "    return img1,img2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1oaeY14Rw-k"
      },
      "source": [
        "# Question 1: Matching an object in a pair of images (60%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0aa7CxpRw-l"
      },
      "source": [
        "\n",
        "In this question, the aim is to accurately locate a reference object in a query image, for example:\n",
        "\n",
        "![Books](book.png \"Books\")\n",
        "\n",
        "0. Download and read through the paper [ORB: an efficient alternative to SIFT or SURF](https://www.researchgate.net/publication/221111151_ORB_an_efficient_alternative_to_SIFT_or_SURF) by Rublee et al. You don't need to understand all the details, but try to get an idea of how it works. ORB combines the FAST corner detector and the BRIEF descriptor. BRIEF is based on similar ideas to the SIFT descriptor we covered week 3, but with some changes for efficiency.\n",
        "\n",
        "1. [Load images] Load the first (reference, query) image pair from the \"book_covers\" category using opencv (e.g. `img=cv2.imread()`). Check the parameter option in \"\n",
        "cv2.imread()\" to ensure that you read the gray scale image, since it is necessary for computing ORB features.\n",
        "\n",
        "2. [Detect features] Create opencv ORB feature extractor by `orb=cv2.ORB_create()`. Then you can detect keypoints by `kp = orb.detect(img,None)`, and compute descriptors by `kp, des = orb.compute(img, kp)`. You need to do this for each image, and then you can use `cv2.drawKeypoints()` for visualization.\n",
        "\n",
        "3. [Match features] As ORB is a binary feature, you need to use HAMMING distance for matching, e.g., `bf = cv2.BFMatcher(cv2.NORM_HAMMING)`. Then you are requried to do KNN matching (k=2) by using `bf.knnMatch()`. After that, you are required to use \"ratio_test\". Ratio test was used in SIFT to find good matches and was described in the lecture. By default, you can set `ratio=0.8`.\n",
        "\n",
        "4. [Plot and analyze] You need to visualize the matches by using the `cv2.drawMatches()` function. Also you can change the ratio values, parameters in `cv2.ORB_create()`, and distance functions in `cv2.BFMatcher()`. Please discuss how these changes influence the match numbers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IkPFZ5psV3iE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not load img1. Check the path, filename and current working directory\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@2830.020] global loadsave.cpp:268 findDecoder imread_('/A2_smvs/book_covers/Reference/001.jpg'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@2830.020] global loadsave.cpp:268 findDecoder imread_('/A2_smvs/book_covers/Reference/001.jpg'): can't open/read file: check file path/integrity\n",
            "3239.66s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/riley/compvis/compVis\n",
            "Could not load img2. Check the path, filename and current working directory\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@2835.201] global loadsave.cpp:268 findDecoder imread_('/A2_smvs/book_covers/Query/001.jpg'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@2835.201] global loadsave.cpp:268 findDecoder imread_('/A2_smvs/book_covers/Query/001.jpg'): can't open/read file: check file path/integrity\n",
            "3244.83s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/riley/compvis/compVis\n"
          ]
        }
      ],
      "source": [
        "# load images as grey scale\n",
        "img1 = cv2.imread('/A2_smvs/book_covers/Reference/001.jpg', 0)\n",
        "if not np.shape(img1):\n",
        "  # Error message and print current working dir\n",
        "  print(\"Could not load img1. Check the path, filename and current working directory\\n\")\n",
        "  !pwd\n",
        "img2 = cv2.imread(\"/A2_smvs/book_covers/Query/001.jpg\", 0)\n",
        "if not np.shape(img2):\n",
        "  # Error message and print current working dir\n",
        "  print(\"Could not load img2. Check the path, filename and current working directory\\n\")\n",
        "  !pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxgtkUEyRw-l"
      },
      "outputs": [],
      "source": [
        "# Your code for descriptor matching tests here\n",
        "\n",
        "# compute detector and descriptor, see (2) above\n",
        "\n",
        "\n",
        "# find the keypoints and descriptors with ORB, see (2) above\n",
        "\n",
        "# draw keypoints, see (2) above\n",
        "\n",
        "\n",
        "# create BFMatcher object, see (3) above\n",
        "\n",
        "\n",
        "# Match descriptors, see (3) above\n",
        "\n",
        "\n",
        "# Apply ratio test, see (3) above\n",
        "#good = []\n",
        "#for m,n in matches:\n",
        "\n",
        "\n",
        "# draw matches, see (4) above\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kqoJtPyRw-m"
      },
      "source": [
        "***Your explanation of what you have done, and your results, here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUkccmvRw-m"
      },
      "source": [
        "5. Estimate a homography transformation based on the matches, using `cv2.findHomography()`. Display the transformed outline of the first reference book cover image on the query image, to see how well they match.\n",
        "\n",
        "    - We provide a function `draw_outline()` to help with the display, but you may need to edit it for your needs.\n",
        "    - Try the 'least squre method' option to compute homography, and visualize the inliers by using `cv2.drawMatches()`. Explain your results.\n",
        "    - Again, you don't need to compare results numerically at this stage. Comment on what you observe visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEB3wHzDRw-n"
      },
      "outputs": [],
      "source": [
        "# Create src_pts and dst_pts as float arrays to be passed into cv2.,findHomography\n",
        "\n",
        "\n",
        "# using cv2 standard method, see (3) above\n",
        "\n",
        "# draw frame\n",
        "\n",
        "# draw inliers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgYKUk0vRw-o"
      },
      "source": [
        "***Your explanation of results here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DzFK-WJRw-o"
      },
      "source": [
        "Try the RANSAC option to compute homography. Change the RANSAC parameters, and explain your results. Print and analyze the inlier numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovjvBHr8Rw-o"
      },
      "outputs": [],
      "source": [
        "# Your code to display book location after RANSAC here\n",
        "\n",
        "# using RANSAC\n",
        "\n",
        "# draw frame\n",
        "\n",
        "\n",
        "# draw inliers\n",
        "\n",
        "\n",
        "# inlier number\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqZFTj6fRw-p"
      },
      "source": [
        "***Your explanation of what you have tried, and results here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh2FlHznRw-p"
      },
      "source": [
        "6. Finally, try matching several different image pairs from the data provided, including at least one success and one failure case. For the failure case, test and explain what step in the feature matching has failed, and try to improve it. Display and discuss your findings.\n",
        "    1. Hint 1: In general, the book covers should be the easiest to match, while the landmarks are the hardest.\n",
        "    2. Hint 2: Explain why you chose each example shown, and what parameter settings were used.\n",
        "    3. Hint 3: Possible failure points include the feature detector, the feature descriptor, the matching strategy, or a combination of these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPKKm0p7Rw-p"
      },
      "outputs": [],
      "source": [
        "# Your results for other image pairs here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m21adz-lRw-q"
      },
      "source": [
        "***Your explanation of results here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6_2eGq1Rw-q"
      },
      "source": [
        "# Question 2: What am I looking at? (40%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1w4nmRiRw-q",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "\n",
        "<!--One application of feature matching is image retrieval. The goal of image retrieval is, given a query image of an object, to find all images in a database containing the same object, and return the results in ranked order (like a Google search). This is a huge research area but we will implement a very basic version of the problem based on the small dataset provided.-->\n",
        "\n",
        "In this question, the aim is to identify an \"unknown\" object depicted in a query image, by matching it to multiple reference images, and selecting the highest scoring match. Since we only have one reference image per object, there is at most one correct answer. This is useful for example if you want to automatically identify a book from a picture of its cover, or a painting or a geographic location from an unlabelled photograph of it.\n",
        "\n",
        "The steps are as follows:\n",
        "\n",
        "1. Select a set of reference images and their corresponding query images.\n",
        "\n",
        "    1. Hint 1: Start with the book covers, or just a subset of them.\n",
        "    3. Hint 2: This question can require a lot of computation to run from start to finish, so cache intermediate results (e.g. feature descriptors) where you can.\n",
        "    \n",
        "2. Choose one query image corresponding to one of your reference images. Use RANSAC to match your query image to each reference image, and count the number of inlier matches found in each case. This will be the matching score for that image.\n",
        "\n",
        "3. Identify the query object. This is the identity of the reference image with the highest match score, or \"not in dataset\" if the maximum score is below a threshold.\n",
        "\n",
        "4. Repeat steps 2-3 for every query image and report the overall accuracy of your method (that is, the percentage of query images that were correctly matched in the dataset). Discussion of results should include both overall accuracy and individual failure cases.\n",
        "\n",
        "    1. Hint 1: In case of failure, what ranking did the actual match receive? If we used a \"top-k\" accuracy measure, where a match is considered correct if it appears in the top k match scores, would that change the result?\n",
        "\n",
        "<!--Code to implement this algorithm should mostly be written in a supporting file such as a2code.py. Call your code and display outputs in the notebook below.-->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmYir3PdRw-q"
      },
      "outputs": [],
      "source": [
        "# Your code to iddntify query objects and measure search accuracy for data set here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf8hSLunRw-r"
      },
      "source": [
        "***Your explanation of what you have done, and your results, here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsofuMOSRw-r"
      },
      "source": [
        "5. Choose some extra query images of objects that do not occur in the reference dataset. Repeat step 4 with these images added to your query set. Accuracy is now measured by the percentage of query images correctly identified in the dataset, or correctly identified as not occurring in the dataset. Report how accuracy is altered by including these queries, and any changes you have made to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aaMXG0hRw-r"
      },
      "outputs": [],
      "source": [
        "# Your code to run extra queries and display results here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cqzNXdVRw-r"
      },
      "source": [
        "***Your explanation of results and any changes made here***\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JME2C25JRw-s"
      },
      "source": [
        "\n",
        "6. Repeat step 4 and 5 for at least one other set of reference images from museum_paintings or landmarks, and compare the accuracy obtained. Analyse both your overall result and individual image matches to diagnose where problems are occurring, and what you could do to improve performance. Test at least one of your proposed improvements and report its effect on accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys3VnXo6Rw-s"
      },
      "outputs": [],
      "source": [
        "# Your code to search images and display results here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFuDgvahRw-s"
      },
      "source": [
        "***Your description of what you have done, and explanation of results, here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1uwT6Tmvxpi"
      },
      "source": [
        "## Question 3: FUndametal Matrix, Epilines and Retrival (optional, assesed for granting up to 25% bonus marks for the A2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lGnUzb9vxpi"
      },
      "source": [
        "In this question, the aim is to accuractely estimate the fundamental matrix given two views of a scene, visualise the corresponding epipolar lines and use the inlier count of fundametal matrix for retrival.\n",
        "\n",
        "The steps are as follows:\n",
        "\n",
        "1. Select two images of the same scene (query and reference) from the landmark dataset and find the matches as you have done in Question 1 (1.1-1.4).\n",
        "2. Compute fundametal metrix with good matches (after appying ratio test) using the opencv function cv.findFundamentalMat(). Use both 8 point algorithm and RANSAC assisted 8 point algorithm to compute fundamental matrix.\n",
        " 1. Hint: You need minimum 8 matches to be able to use the function. Ignore pairs where 8 mathes are not found.\n",
        "3. Visualise the epipolar lines for the matched features and analyse the results. You can use openCV function cv.computeCorrespondEpilines() to estimate the epilines. We have provided the code for drawing these epilines in function drawlines() that you can modify as required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52zwi6ffvxpi"
      },
      "outputs": [],
      "source": [
        "# Your code to find fundamental matrices with and without RANSAC goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56jyN0V7vxpi"
      },
      "outputs": [],
      "source": [
        "# Your code to visualise the epipolar lines for both cases for an example goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkjdcLpN5qWm"
      },
      "source": [
        "***Your visualization for epilines goes here***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g82qzQJ6QCN"
      },
      "source": [
        "4. Repeat the steps for some examples from the landmarks datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0lWJBa56FN8"
      },
      "outputs": [],
      "source": [
        " # Your code for additional images goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDaekAjv6jdD"
      },
      "source": [
        "***Your visualization for additional epillines goes here***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOSkHKoJ62a-"
      },
      "source": [
        "5. Find a query from landmarks data for which the retrival in Q2 failed. Attempt the retrival with replacing the Homography + RANSAC method of Q2 to Fundamental Matrix + RANSAC method using code written above.\n",
        "Does the change of the model makes retraival successful? Analyse and comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5PoldAg7gkt"
      },
      "outputs": [],
      "source": [
        "# Your code for retrival goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_URoCyaG7kbK"
      },
      "source": [
        "***Your analysis goes here***\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
