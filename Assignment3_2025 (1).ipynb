{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMOJD0_jdzzg"
      },
      "source": [
        "## Computer vision 2025 Assignment 3\n",
        "## Deep Learning for Perception Tasks\n",
        "\n",
        "This assignment contains 2 questions. The first question probes understanding of deep learning for classification. The second question requires you to write a short description of a Computer Vision method. You wil need to submit two separate PDF files, one for each question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# Set up\n",
        "import numpy as np # This is for mathematical operations\n",
        "\n",
        "# this is used in plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%reload_ext autoreload\n",
        "\n",
        "#### Tutorial Code\n",
        "####PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset.\n",
        "#####Dataset stores samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset.\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download training data from open datasets.\n",
        "##Every TorchVision Dataset includes two arguments:\n",
        "##transform and target_transform to modify the samples and labels respectively.\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Question 1: A simple classifier, 20 marks (60% of the assignment)\n",
        "\n",
        "For this exercise, we provide demo code showing how to train a network on a small dataset called Fashion-MNIST. Please run through the code \"tutorial-style\" to get a sense of what it is doing. Then use the code alongside lecture notes and other resources to understand how to use pytorch libraries to implement, train and use a neural network.\n",
        "\n",
        "For the Fashion-MNIST dataset the labels from 0-9 correspond to various clothing classes so you might find it convenient to create a python list as follows:\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "You will need to answer various questions about the system, how it operates, the results of experiments with it and make modifications to it yourself. You can change the training scheme and the network structure.\n",
        "\n",
        "Organize your own text and code cell to show the answer of each question below.\n",
        "\n",
        "Detailed requirements:\n",
        "\n",
        "### Q1.1 (1 point)\n",
        "\n",
        "Extract 3 images of different types of clothing from the training dataset, print out the size/shape of the training images, and display the three with their corresponding labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (image, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images):\n\u001b[1;32m     14\u001b[0m   plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "images = []\n",
        "for image, label in training_data:\n",
        "  images.append((image, label))\n",
        "  \n",
        "  if len(images) == 3:\n",
        "    break\n",
        "  \n",
        "print(images[0].shape)\n",
        "\n",
        "for i, (image, label) in enumerate(images):\n",
        "  plt.subplot(1, 3, i+1)\n",
        "  plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "  plt.title(f\"{class_names[label]}\")\n",
        "  plt.show\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Q1.2 (2 point) \n",
        "\n",
        "Run the training code for 10 epochs, for different values of the learning rate. Fill in the table below and plot the loss curves for each experiment:\n",
        "\n",
        "|Lr|Accuracy|\n",
        "|---|---|\n",
        "|1   |      |\n",
        "|0.1|          |\n",
        "|0.01|         |\n",
        "|0.001  |        |\n",
        "\n",
        "\n",
        "### Q1.3 (3 point) \n",
        "Report the number of epochs when the network converges (or number of epochs for the best accuracy, if it fails to converge). Fill in the table below and plot the loss curve for each experiment. Please run the code for more than 10 epochs (e.g., 50 or 100) and report when you observe convergence.:\n",
        "\n",
        "|Lr|Accuracy|Epoch|\n",
        "|---|---|---|\n",
        "|1   |      |     |\n",
        "|0.1|          |    |\n",
        "|0.01|         |    |\n",
        "|0.001  |        |     |\n",
        "\n",
        "\n",
        "### Q1.4 (2 points) \n",
        "\n",
        "Compare the results in table 1 and table 2, what is your observation and your understanding of learning rate?\n",
        "\n",
        "\n",
        "### Q1.5 (5 points) \n",
        "\n",
        "Build a wider network by modifying the code that constructs the network so that the hidden layer(s) contain more perceptrons, and record the accuracy along with the number of trainable parameters in your model.  Now modify the original network to be deeper instead of wider (i.e. by adding more hidden layers). Record your accuracy and network size findings. Plot the loss curve for each experiment. Also plot the test accuracy and loss for both the wider and deeper architectures and discuss what you observe. Write down your conclusions about changing the network structure?  \n",
        "\n",
        "|Structures|Accuracy|Parameters|\n",
        "|---|---|---|\n",
        "|Base   |      ||\n",
        "|Deeper|          ||\n",
        "|Wider|         ||\n",
        "\n",
        "\n",
        "### Q1.6 (2 points) \n",
        "\n",
        "Calculate the mean of the gradients of the loss to all trainable parameters. Plot the gradients curve for the first 100 training steps. What are your observations? Note that this gradients will be saved with the training weight automatically after you call loss.backwards(). Hint: the mean of the gradients decrease.\n",
        "\n",
        "For more explanation of q1.7, you could refer to the following simple instructions: https://colab.research.google.com/drive/1XAsyNegGSvMf3_B6MrsXht7-fHqtJ7OW?usp=sharing\n",
        "\n",
        "### Q1.7 (5 points) \n",
        "\n",
        "Modify the network structure and training/test to use a small convolutional neural network instead of an MLP. Discuss your findings with regard to convergence, accuracy and number of parameters, relative to MLPs.  \n",
        "\n",
        "Hint: Look at the structure of the CNN in the Workshop 3 examples.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q2 Optional Bonus Question  (5 points, upto 20% bonus marks of the assignment) \n",
        "\n",
        "Experiment with different activation functions (ReLU, Tanh, Sigmoid) and analyse their impact on training performance.(2 points) \n",
        "\n",
        "In particular, focus your analysis on the Sigmoid activation function and discuss your finding of training with and without Xavier initialization. You may use the provided code for Xavier initialisation for this. (1 points)\n",
        "\n",
        "Additionally, plot both the gradient and loss curves for your experiments. For gradient analysis, you may select one representative layer to monitor throughout training and briefly explain your choice. (1 points)\n",
        "\n",
        "Discuss how gradients and loss behave across the network for different activation functions and initialisation methods if you see any difference. (1 points) \n",
        "\n",
        "Note: the bonus marks wont replace marks for Q3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S16_96RS3O1H"
      },
      "source": [
        "# Question 3: Proposal for Practical Applications (40%)\n",
        "\n",
        "In this part of the assignment you need to write a report about an application of a computer vision algorithm or technique. This can either be an application that you are aware of and possibly even use, such as a phone app, or it could be speculation -- an application that you think would benefit from using computer vision.\n",
        "\n",
        "Begin by choosing a particular CV idea, method or problem area, such as:\n",
        "\n",
        "a. removing noise in an image\n",
        "\n",
        "b. increasing the resolution of an image\n",
        "\n",
        "c. detecting and/or identifying objects in an image\n",
        "\n",
        "d. segmenting images into constituents parts\n",
        "\n",
        "e. estimating the depth of an object from one or more images\n",
        "\n",
        "f. estimating the motion of two objects in different frames\n",
        "\n",
        "g. others\n",
        "\n",
        "Now think about various ways your chosen technique could be used. Here is a list of possible applications you could consider, but you are not restricted to this list, and there will be credit given for sensible invention outside this list (but no penalty if you don't want to be \"inventive\"): image editing systems in your phone; enhancement of images from old film; obstacle detection and avoidance for a domestic robot; facial recognition for phone security; cancer detection; person tracking and re-identification in security cameras; sport decision review systems; road-sign detection and interpretation for self-driving cars.\n",
        "\n",
        "This is a little bit back-to-front from what might happen in real life in which the application usually motivates the solution, but the emphasis here is on an understanding of the CV technique.\n",
        "\n",
        "You need to write a short report (800 words max) in which you do the following:\n",
        "1. Clearly define the CV problem/area and describe its application scenarios\n",
        "2. Briefly describe a solution based on image processing, computer vision and/or machine learning.\n",
        "3. Discuss the advantages and the limitations of this method in various application scenarios.\n",
        "4. It is important that you will define a useful metric to evaluate the performance of your method and discuss its tradeoff specific to the problem you have chosen.\n",
        "5. You are welcome to cite existing work and take inspiration form literature addressing the problem you choose.\n",
        "\n",
        "Hint1: Submit an individual pdf report for question 2.\n",
        "\n",
        "Hint2: Organise your report well\n",
        "\n",
        "Hint3: You can use diagrams, flow charts or other figures in your report for better understanding of your solution.  \n",
        "\n",
        "** For Q3, you do not need to implement your solution; just write the proposal/report and submit it as a separate PDF **\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The tutorial style code starts here, reuse parts as you like  while writing your report #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-sqkIpLjpVsh"
      },
      "outputs": [],
      "source": [
        "import numpy as np # This is for mathematical operations\n",
        "\n",
        "# this is used in plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%reload_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1wy3xhEx_x-1"
      },
      "outputs": [],
      "source": [
        "#### Tutorial Code\n",
        "####PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset.\n",
        "#####Dataset stores samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset.\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download training data from open datasets.\n",
        "##Every TorchVision Dataset includes two arguments:\n",
        "##transform and target_transform to modify the samples and labels respectively.\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNI4IusI_1ol"
      },
      "source": [
        "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset and supports automatic batching, sampling, shuffling, and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nQZ5l5Zs_4C3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L1vl5rC52Un"
      },
      "source": [
        "Add in a code cell to inspect the training data, as per Q1.1. Each element of the training_data structure has a grayscale image (which you can use plt.imshow(img[0,:,:]) to display, just like you did in previous assignments.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KpEhLSHg4Idw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(close=None, block=None)>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJo5JREFUeJzt3X9w1PWdx/HXJpAlIcliCPlVAwQUseWHJ0L4oYgSgfTOSqHFX+1AqyK54FXR0+HGSq03kxbvquOVgufdQO2ICnMCo2PpIJCgFbQgFL1TCjQUMCT8qNkN+bEJ2e/9wbh1Jfz4fNzkkx/Px8zOkN3vK98PX77Ji292816f53meAADoYAmuFwAA6JkoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIPQoq1atks/n06FDh4yzU6ZM0YgRI+K6nsGDB2vevHlx/ZxAV0EBAd1YTU2NfvCDHygrK0vJycm69tprtXbtWtfLAiRRQEC3FQqFdP311+t//ud/dP/99+vf/u3flJaWpjlz5mj16tWulweol+sFAGgfzz//vA4cOKDNmzfr5ptvliSVlJRo/Pjxevjhh/Wd73xHSUlJjleJnowrIPR4GzZs0N///d8rLy9Pfr9fQ4cO1VNPPaXW1tY2t9+1a5cmTpyo5ORkFRQUaMWKFedsEw6HtWTJEl1xxRXy+/3Kz8/Xo48+qnA43N5/nai3335bAwYMiJaPJCUkJGjOnDmqrq5WRUVFh60FaAtXQOjxVq1apdTUVC1atEipqanasmWLnnjiCYVCIT399NMx23722Wf65je/qTlz5ujOO+/UmjVrVFJSoqSkJP3whz+UJEUiEX3rW9/SO++8o/nz5+vqq6/Whx9+qGeeeUZ/+tOftH79+vOuJRKJ6K9//eslrTsQCKh3797nfTwcDis5Ofmc+1NSUiSdLdJbbrnlkvYFtAsP6EFWrlzpSfIqKyuj9zU0NJyz3f333++lpKR4TU1N0ftuvPFGT5L37//+79H7wuGwd80113hZWVlec3Oz53me95vf/MZLSEjw3n777ZjPuWLFCk+S9/vf/z5636BBg7y5c+dGP66srPQkXdJt69atF/y7PvDAA15CQoJ36NChmPvvuOMOT5K3cOHCC+aB9sYVEHq8L14l1NXVKRwO64YbbtDzzz+vTz75RKNHj44+3qtXL91///3Rj5OSknT//ferpKREu3bt0vjx47V27VpdffXVGj58uE6ePBnd9vMfhW3dulUTJ05scy05OTnatGnTJa37i+tqy7333qsVK1Zozpw5euaZZ5Sdna01a9Zo3bp1kqTGxsZL2g/QXigg9Hj/+7//q8cff1xbtmxRKBSKeSwYDMZ8nJeXp759+8bcN2zYMEnSoUOHNH78eO3fv18ff/yxBgwY0Ob+jh8/ft619OnTR0VFRTZ/jXOMGjVKq1ev1oIFCzRp0iRJZwvu2WefVUlJiVJTU+OyH8AWBYQerba2VjfeeKPS09P105/+VEOHDlWfPn30wQcf6LHHHlMkEjH+nJFIRCNHjtQvfvGLNh/Pz88/b7a1tVUnTpy4pP1kZGRc9FVs3/nOd/Stb31Lf/zjH9Xa2qprr71W5eXlkv5WnIArFBB6tPLycp06dUqvvfaaJk+eHL2/srKyze2rqqpUX18fcxX0pz/9SdLZqQaSNHToUP3xj3/U1KlT5fP5jNZz5MgRFRQUXNK2W7du1ZQpUy66XVJSksaOHRv9+K233pKkuF1pAbYoIPRoiYmJkiTP86L3NTc361e/+lWb2585c0bPP/+8Fi1aFN32+eef14ABAzRmzBhJ0pw5c/Tmm2/qhRde0Pz582PyjY2NikQi5/wY73PxfA6oLfv379eKFSv0D//wD1wBwTkKCD3axIkTddlll2nu3Ln6p3/6J/l8Pv3mN7+JKaQvysvL089//nMdOnRIw4YN06uvvqo9e/boP//zP6Mvif7+97+vNWvWaMGCBdq6dasmTZqk1tZWffLJJ1qzZo1+97vf6brrrmvz88fzOSBJ+vrXv67vfve7GjhwoCorK7V8+XJlZGS0+btLQEejgNCj9e/fX2+88YYefvhhPf7447rsssv0ve99T1OnTtX06dPP2f6yyy7Tr3/9az3wwAN64YUXlJ2drV/+8pe67777otskJCRo/fr1euaZZ/Tiiy9q3bp1SklJ0ZAhQ/SjH/2oQ688Ro8erZUrV6qmpkaZmZmaM2eOnnzySWVlZXXYGoDz8Xnn+68eAADtiFE8AAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA40el+DygSiaiqqkppaWnGY0wAAO55nqe6ujrl5eUpIeH81zmdroCqqqouOKwRANA1HDlyRJdffvl5H+90P4JLS0tzvQQAQBxc7Pt5uxXQsmXLNHjwYPXp00eFhYV6//33LynHj90AoHu42PfzdimgV199VYsWLdKSJUv0wQcfaPTo0Zo+ffoF34gLANDDtMf7fI8bN84rLS2Nftza2url5eV5ZWVlF80Gg8GY973nxo0bN25d8xYMBi/4/T7uV0DNzc3atWtXzEj5hIQEFRUVafv27edsHw6HFQqFYm4AgO4v7gV08uRJtba2Kjs7O+b+7OxsVVdXn7N9WVmZAoFA9MYr4ACgZ3D+KrjFixcrGAxGb0eOHHG9JABAB4j77wFlZmYqMTFRNTU1MffX1NQoJyfnnO39fr/8fn+8lwEA6OTifgWUlJSkMWPGaPPmzdH7IpGINm/erAkTJsR7dwCALqpdJiEsWrRIc+fO1XXXXadx48bp2WefVX19vX7wgx+0x+4AAF1QuxTQ7bffrhMnTuiJJ55QdXW1rrnmGm3cuPGcFyYAAHoun+d5nutFfFEoFFIgEHC9DADAVxQMBpWenn7ex52/Cg4A0DNRQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJ3q5XgDQmfh8PuOM53ntsJJzpaWlGWeuv/56q3399re/tcqZsjneiYmJxpkzZ84YZzo7m2Nnq73Oca6AAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJhpECX5CQYP5/stbWVuPMFVdcYZy59957jTONjY3GGUmqr683zjQ1NRln3n//feNMRw4WtRn4aXMO2eynI4+D6QBYz/MUiUQuuh1XQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBMNIgS8wHboo2Q0jvfnmm40zRUVFxpmjR48aZyTJ7/cbZ1JSUowzt9xyi3Hmv/7rv4wzNTU1xhnp7FBNUzbng43U1FSr3KUMCf2yhoYGq31dDFdAAAAnKCAAgBNxL6Cf/OQn8vl8Mbfhw4fHezcAgC6uXZ4D+sY3vqG33nrrbzvpxVNNAIBY7dIMvXr1Uk5OTnt8agBAN9EuzwHt379feXl5GjJkiO6++24dPnz4vNuGw2GFQqGYGwCg+4t7ARUWFmrVqlXauHGjli9frsrKSt1www2qq6trc/uysjIFAoHoLT8/P95LAgB0QnEvoOLiYn33u9/VqFGjNH36dL355puqra3VmjVr2tx+8eLFCgaD0duRI0fivSQAQCfU7q8O6Nevn4YNG6YDBw60+bjf77f6pTcAQNfW7r8HdPr0aR08eFC5ubntvSsAQBcS9wJ65JFHVFFRoUOHDundd9/Vt7/9bSUmJurOO++M964AAF1Y3H8Ed/ToUd155506deqUBgwYoOuvv147duzQgAED4r0rAEAXFvcCeuWVV+L9KYEO09zc3CH7GTt2rHFm8ODBxhmb4aqSlJBg/sOR3/3ud8aZv/u7vzPOLF261Dizc+dO44wkffjhh8aZjz/+2Dgzbtw444zNOSRJ7777rnFm+/btRtt7nndJv1LDLDgAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcKLd35AOcMHn81nlPM8zztxyyy3Gmeuuu844c763tb+Qvn37GmckadiwYR2S+cMf/mCcOd+bW15IamqqcUaSJkyYYJyZNWuWcaalpcU4Y3PsJOnee+81zoTDYaPtz5w5o7fffvui23EFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACd8ns3433YUCoUUCARcLwPtxHZKdUex+XLYsWOHcWbw4MHGGRu2x/vMmTPGmebmZqt9mWpqajLORCIRq3198MEHxhmbad02x3vGjBnGGUkaMmSIceZrX/ua1b6CwaDS09PP+zhXQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgRC/XC0DP0slm38bFZ599ZpzJzc01zjQ2Nhpn/H6/cUaSevUy/9aQmppqnLEZLJqcnGycsR1GesMNNxhnJk6caJxJSDC/FsjKyjLOSNLGjRutcu2BKyAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIJhpMBXlJKSYpyxGT5pk2loaDDOSFIwGDTOnDp1yjgzePBg44zNQFufz2eckeyOuc350NraapyxHbCan59vlWsPXAEBAJyggAAAThgX0LZt23TrrbcqLy9PPp9P69evj3nc8zw98cQTys3NVXJysoqKirR///54rRcA0E0YF1B9fb1Gjx6tZcuWtfn40qVL9dxzz2nFihV677331LdvX02fPt3qjacAAN2X8YsQiouLVVxc3OZjnufp2Wef1eOPP67bbrtNkvTiiy8qOztb69ev1x133PHVVgsA6Dbi+hxQZWWlqqurVVRUFL0vEAiosLBQ27dvbzMTDocVCoVibgCA7i+uBVRdXS1Jys7Ojrk/Ozs7+tiXlZWVKRAIRG+d6SWCAID24/xVcIsXL1YwGIzejhw54npJAIAOENcCysnJkSTV1NTE3F9TUxN97Mv8fr/S09NjbgCA7i+uBVRQUKCcnBxt3rw5el8oFNJ7772nCRMmxHNXAIAuzvhVcKdPn9aBAweiH1dWVmrPnj3KyMjQwIED9eCDD+pf//VfdeWVV6qgoEA//vGPlZeXp5kzZ8Zz3QCALs64gHbu3Kmbbrop+vGiRYskSXPnztWqVav06KOPqr6+XvPnz1dtba2uv/56bdy4UX369InfqgEAXZ7Ps5ns145CoZACgYDrZaCd2AyFtBkIaTPcUZJSU1ONM7t37zbO2ByHxsZG44zf7zfOSFJVVZVx5svP/V6KiRMnGmdshp7aDAiVpKSkJONMXV2dccbme57tC7ZszvF77rnHaPvW1lbt3r1bwWDwgs/rO38VHACgZ6KAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJ47djAL4Km+HriYmJxhnbadi33367ceZ87/Z7ISdOnDDOJCcnG2cikYhxRpL69u1rnMnPzzfONDc3G2dsJny3tLQYZySpVy/zb5E2/079+/c3zixbtsw4I0nXXHONccbmOFwKroAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAmGkaJD2Qw1tBlYaeujjz4yzoTDYeNM7969jTMdOZQ1KyvLONPU1GScOXXqlHHG5tj16dPHOCPZDWX97LPPjDNHjx41ztx1113GGUl6+umnjTM7duyw2tfFcAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE706GGkPp/PKmczFDIhwbzrbdbX0tJinIlEIsYZW2fOnOmwfdl48803jTP19fXGmcbGRuNMUlKSccbzPOOMJJ04ccI4Y/N1YTMk1OYct9VRX082x27UqFHGGUkKBoNWufbAFRAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAONFthpHaDPNrbW212ldnH6jZmU2ePNk4M3v2bOPMpEmTjDOS1NDQYJw5deqUccZmsGivXuZfrrbnuM1xsPka9Pv9xhmbAaa2Q1ltjoMNm/Ph9OnTVvuaNWuWceb111+32tfFcAUEAHCCAgIAOGFcQNu2bdOtt96qvLw8+Xw+rV+/PubxefPmyefzxdxmzJgRr/UCALoJ4wKqr6/X6NGjtWzZsvNuM2PGDB07dix6e/nll7/SIgEA3Y/xs5rFxcUqLi6+4DZ+v185OTnWiwIAdH/t8hxQeXm5srKydNVVV6mkpOSCrxIKh8MKhUIxNwBA9xf3ApoxY4ZefPFFbd68WT//+c9VUVGh4uLi874ctKysTIFAIHrLz8+P95IAAJ1Q3H8P6I477oj+eeTIkRo1apSGDh2q8vJyTZ069ZztFy9erEWLFkU/DoVClBAA9ADt/jLsIUOGKDMzUwcOHGjzcb/fr/T09JgbAKD7a/cCOnr0qE6dOqXc3Nz23hUAoAsx/hHc6dOnY65mKisrtWfPHmVkZCgjI0NPPvmkZs+erZycHB08eFCPPvqorrjiCk2fPj2uCwcAdG3GBbRz507ddNNN0Y8/f/5m7ty5Wr58ufbu3atf//rXqq2tVV5enqZNm6annnrKauYTAKD78nm2U/raSSgUUiAQcL2MuMvIyDDO5OXlGWeuvPLKDtmPZDfUcNiwYcaZcDhsnElIsPvpcktLi3EmOTnZOFNVVWWc6d27t3HGZsilJPXv398409zcbJxJSUkxzrz77rvGmdTUVOOMZDc8NxKJGGeCwaBxxuZ8kKSamhrjzNVXX221r2AweMHn9ZkFBwBwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACfi/pbcrowfP94489RTT1nta8CAAcaZfv36GWdaW1uNM4mJicaZ2tpa44wknTlzxjhTV1dnnLGZsuzz+YwzktTY2GicsZnOPGfOHOPMzp07jTNpaWnGGcluAvngwYOt9mVq5MiRxhnb43DkyBHjTENDg3HGZqK67YTvQYMGWeXaA1dAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBEpx1GmpCQYDRQ8rnnnjPeR25urnFGshsSapOxGWpoIykpySpn83eyGfZpIxAIWOVsBjX+7Gc/M87YHIeSkhLjTFVVlXFGkpqamowzmzdvNs78+c9/Ns5ceeWVxpn+/fsbZyS7Qbi9e/c2ziQkmF8LtLS0GGck6cSJE1a59sAVEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA44fM8z3O9iC8KhUIKBAK6++67jYZk2gyEPHjwoHFGklJTUzsk4/f7jTM2bIYnSnYDP48cOWKcsRmoOWDAAOOMZDcUMicnxzgzc+ZM40yfPn2MM4MHDzbOSHbn65gxYzokY/NvZDNU1HZftsN9TZkMa/4im6/38ePHG20fiUT06aefKhgMKj09/bzbcQUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE70cr2A8zlx4oTR0DybIZdpaWnGGUkKh8PGGZv12QyEtBmEeKFhgRfy17/+1Tjzl7/8xThjcxwaGxuNM5LU1NRknDlz5oxxZt26dcaZDz/80DhjO4w0IyPDOGMz8LO2ttY409LSYpyx+TeSzg7VNGUz7NNmP7bDSG2+RwwbNsxo+zNnzujTTz+96HZcAQEAnKCAAABOGBVQWVmZxo4dq7S0NGVlZWnmzJnat29fzDZNTU0qLS1V//79lZqaqtmzZ6umpiauiwYAdH1GBVRRUaHS0lLt2LFDmzZtUktLi6ZNm6b6+vroNg899JBef/11rV27VhUVFaqqqtKsWbPivnAAQNdm9CKEjRs3xny8atUqZWVladeuXZo8ebKCwaD++7//W6tXr9bNN98sSVq5cqWuvvpq7dixw/hd9QAA3ddXeg4oGAxK+tsrZnbt2qWWlhYVFRVFtxk+fLgGDhyo7du3t/k5wuGwQqFQzA0A0P1ZF1AkEtGDDz6oSZMmacSIEZKk6upqJSUlqV+/fjHbZmdnq7q6us3PU1ZWpkAgEL3l5+fbLgkA0IVYF1Bpaak++ugjvfLKK19pAYsXL1YwGIzebH5fBgDQ9Vj9IurChQv1xhtvaNu2bbr88suj9+fk5Ki5uVm1tbUxV0E1NTXKyclp83P5/X75/X6bZQAAujCjKyDP87Rw4UKtW7dOW7ZsUUFBQczjY8aMUe/evbV58+boffv27dPhw4c1YcKE+KwYANAtGF0BlZaWavXq1dqwYYPS0tKiz+sEAgElJycrEAjonnvu0aJFi5SRkaH09HQ98MADmjBhAq+AAwDEMCqg5cuXS5KmTJkSc//KlSs1b948SdIzzzyjhIQEzZ49W+FwWNOnT9evfvWruCwWANB9+DzP81wv4otCoZACgYBGjhypxMTES8698MILxvs6efKkcUaS+vbta5zp37+/ccZmUOPp06eNMzbDEyWpVy/zpxBthi6mpKQYZ2wGmEp2xyIhwfy1PDZfdl9+deml+OIviZuwGeb62WefGWdsnv+1+bq1GWAq2Q0xtdlXcnKyceZ8z6tfjM0Q05deeslo+3A4rF/+8pcKBoMXHHbMLDgAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4YfWOqB3hww8/NNr+tddeM97HD3/4Q+OMJFVVVRln/vznPxtnmpqajDM2U6Btp2HbTPBNSkoyzphMRf9cOBw2zkhSa2urccZmsnVDQ4Nx5tixY8YZ22H3NsfBZjp6R53jzc3NxhnJbiK9TcZmgrbNpG5J57yR6KWoqakx2v5SjzdXQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADghM+znVbYTkKhkAKBQIfsq7i42Cr3yCOPGGeysrKMMydPnjTO2AxCtBk8KdkNCbUZRmoz5NJmbZLk8/mMMzZfQjYDYG0yNsfbdl82x86GzX5Mh2l+FTbHPBKJGGdycnKMM5K0d+9e48ycOXOs9hUMBpWenn7ex7kCAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnOu0wUp/PZzR00GaYX0e66aabjDNlZWXGGZuhp7bDXxMSzP//YjMk1GYYqe2AVRvHjx83zth82X366afGGduvi9OnTxtnbAfAmrI5di0tLVb7amhoMM7YfF1s2rTJOPPxxx8bZyTp3XfftcrZYBgpAKBTooAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATnXYYKTrO8OHDrXKZmZnGmdraWuPM5Zdfbpw5dOiQcUayG1p58OBBq30B3R3DSAEAnRIFBABwwqiAysrKNHbsWKWlpSkrK0szZ87Uvn37YraZMmVK9L18Pr8tWLAgrosGAHR9RgVUUVGh0tJS7dixQ5s2bVJLS4umTZum+vr6mO3uu+8+HTt2LHpbunRpXBcNAOj6jN5qcuPGjTEfr1q1SllZWdq1a5cmT54cvT8lJUU5OTnxWSEAoFv6Ss8BBYNBSVJGRkbM/S+99JIyMzM1YsQILV68+IJvaxsOhxUKhWJuAIDuz+gK6IsikYgefPBBTZo0SSNGjIjef9ddd2nQoEHKy8vT3r179dhjj2nfvn167bXX2vw8ZWVlevLJJ22XAQDooqx/D6ikpES//e1v9c4771zw9zS2bNmiqVOn6sCBAxo6dOg5j4fDYYXD4ejHoVBI+fn5NkuCJX4P6G/4PSAgfi72e0BWV0ALFy7UG2+8oW3btl30m0NhYaEknbeA/H6//H6/zTIAAF2YUQF5nqcHHnhA69atU3l5uQoKCi6a2bNnjyQpNzfXaoEAgO7JqIBKS0u1evVqbdiwQWlpaaqurpYkBQIBJScn6+DBg1q9erW++c1vqn///tq7d68eeughTZ48WaNGjWqXvwAAoGsyKqDly5dLOvvLpl+0cuVKzZs3T0lJSXrrrbf07LPPqr6+Xvn5+Zo9e7Yef/zxuC0YANA9GP8I7kLy8/NVUVHxlRYEAOgZmIYNAGgXTMMGAHRKFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJzpdAXme53oJAIA4uNj3805XQHV1da6XAACIg4t9P/d5neySIxKJqKqqSmlpafL5fDGPhUIh5efn68iRI0pPT3e0Qvc4DmdxHM7iOJzFcTirMxwHz/NUV1envLw8JSSc/zqnVweu6ZIkJCTo8ssvv+A26enpPfoE+xzH4SyOw1kch7M4Dme5Pg6BQOCi23S6H8EBAHoGCggA4ESXKiC/368lS5bI7/e7XopTHIezOA5ncRzO4jic1ZWOQ6d7EQIAoGfoUldAAIDugwICADhBAQEAnKCAAABOUEAAACe6TAEtW7ZMgwcPVp8+fVRYWKj333/f9ZI63E9+8hP5fL6Y2/Dhw10vq91t27ZNt956q/Ly8uTz+bR+/fqYxz3P0xNPPKHc3FwlJyerqKhI+/fvd7PYdnSx4zBv3rxzzo8ZM2a4WWw7KSsr09ixY5WWlqasrCzNnDlT+/bti9mmqalJpaWl6t+/v1JTUzV79mzV1NQ4WnH7uJTjMGXKlHPOhwULFjhacdu6RAG9+uqrWrRokZYsWaIPPvhAo0eP1vTp03X8+HHXS+tw3/jGN3Ts2LHo7Z133nG9pHZXX1+v0aNHa9myZW0+vnTpUj333HNasWKF3nvvPfXt21fTp09XU1NTB6+0fV3sOEjSjBkzYs6Pl19+uQNX2P4qKipUWlqqHTt2aNOmTWppadG0adNUX18f3eahhx7S66+/rrVr16qiokJVVVWaNWuWw1XH36UcB0m67777Ys6HpUuXOlrxeXhdwLhx47zS0tLox62trV5eXp5XVlbmcFUdb8mSJd7o0aNdL8MpSd66deuiH0ciES8nJ8d7+umno/fV1tZ6fr/fe/nllx2ssGN8+Th4nufNnTvXu+2225ysx5Xjx497kryKigrP887+2/fu3dtbu3ZtdJuPP/7Yk+Rt377d1TLb3ZePg+d53o033uj96Ec/creoS9Dpr4Cam5u1a9cuFRUVRe9LSEhQUVGRtm/f7nBlbuzfv195eXkaMmSI7r77bh0+fNj1kpyqrKxUdXV1zPkRCARUWFjYI8+P8vJyZWVl6aqrrlJJSYlOnTrlekntKhgMSpIyMjIkSbt27VJLS0vM+TB8+HANHDiwW58PXz4On3vppZeUmZmpESNGaPHixWpoaHCxvPPqdNOwv+zkyZNqbW1VdnZ2zP3Z2dn65JNPHK3KjcLCQq1atUpXXXWVjh07pieffFI33HCDPvroI6WlpblenhPV1dWS1Ob58fljPcWMGTM0a9YsFRQU6ODBg/qXf/kXFRcXa/v27UpMTHS9vLiLRCJ68MEHNWnSJI0YMULS2fMhKSlJ/fr1i9m2O58PbR0HSbrrrrs0aNAg5eXlae/evXrssce0b98+vfbaaw5XG6vTFxD+pri4OPrnUaNGqbCwUIMGDdKaNWt0zz33OFwZOoM77rgj+ueRI0dq1KhRGjp0qMrLyzV16lSHK2sfpaWl+uijj3rE86AXcr7jMH/+/OifR44cqdzcXE2dOlUHDx7U0KFDO3qZber0P4LLzMxUYmLiOa9iqampUU5OjqNVdQ79+vXTsGHDdODAAddLcebzc4Dz41xDhgxRZmZmtzw/Fi5cqDfeeENbt26Nef+wnJwcNTc3q7a2Nmb77no+nO84tKWwsFCSOtX50OkLKCkpSWPGjNHmzZuj90UiEW3evFkTJkxwuDL3Tp8+rYMHDyo3N9f1UpwpKChQTk5OzPkRCoX03nvv9fjz4+jRozp16lS3Oj88z9PChQu1bt06bdmyRQUFBTGPjxkzRr179445H/bt26fDhw93q/PhYsehLXv27JGkznU+uH4VxKV45ZVXPL/f761atcr7v//7P2/+/Plev379vOrqatdL61APP/ywV15e7lVWVnq///3vvaKiIi8zM9M7fvy466W1q7q6Om/37t3e7t27PUneL37xC2/37t3eX/7yF8/zPO9nP/uZ169fP2/Dhg3e3r17vdtuu80rKCjwGhsbHa88vi50HOrq6rxHHnnE2759u1dZWem99dZb3rXXXutdeeWVXlNTk+ulx01JSYkXCAS88vJy79ixY9FbQ0NDdJsFCxZ4AwcO9LZs2eLt3LnTmzBhgjdhwgSHq46/ix2HAwcOeD/96U+9nTt3epWVld6GDRu8IUOGeJMnT3a88lhdooA8z/P+4z/+wxs4cKCXlJTkjRs3ztuxY4frJXW422+/3cvNzfWSkpK8r33ta97tt9/uHThwwPWy2t3WrVs9Sefc5s6d63ne2Zdi//jHP/ays7M9v9/vTZ061du3b5/bRbeDCx2HhoYGb9q0ad6AAQO83r17e4MGDfLuu+++bveftLb+/pK8lStXRrdpbGz0/vEf/9G77LLLvJSUFO/b3/62d+zYMXeLbgcXOw6HDx/2Jk+e7GVkZHh+v9+74oorvH/+53/2gsGg24V/Ce8HBABwotM/BwQA6J4oIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMCJ/wc5zXsUoyaf5gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Code cell for training image display\n",
        "image, label = training_data[0]\n",
        "\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"label= {label}\")\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMtCU2LO_9Dk"
      },
      "source": [
        "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the init function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TRSp7pd3_6bS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "       )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nYAnKhOfABZr"
      },
      "outputs": [],
      "source": [
        "###Define the loss function and the optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFZYEHY7ADvS"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L741B0uXAFrf"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A44xKKnjAINf"
      },
      "outputs": [],
      "source": [
        "##Define a test function\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mJLACDm9AKxv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.308336  [    0/60000]\n",
            "loss: 2.284883  [ 6400/60000]\n",
            "loss: 2.263772  [12800/60000]\n",
            "loss: 2.255876  [19200/60000]\n",
            "loss: 2.248618  [25600/60000]\n",
            "loss: 2.218006  [32000/60000]\n",
            "loss: 2.222362  [38400/60000]\n",
            "loss: 2.186428  [44800/60000]\n",
            "loss: 2.190587  [51200/60000]\n",
            "loss: 2.151315  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.6%, Avg loss: 2.145694 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.166863  [    0/60000]\n",
            "loss: 2.145411  [ 6400/60000]\n",
            "loss: 2.088290  [12800/60000]\n",
            "loss: 2.100868  [19200/60000]\n",
            "loss: 2.047711  [25600/60000]\n",
            "loss: 1.995105  [32000/60000]\n",
            "loss: 2.017818  [38400/60000]\n",
            "loss: 1.931812  [44800/60000]\n",
            "loss: 1.955342  [51200/60000]\n",
            "loss: 1.867873  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.0%, Avg loss: 1.866177 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.913951  [    0/60000]\n",
            "loss: 1.869792  [ 6400/60000]\n",
            "loss: 1.751739  [12800/60000]\n",
            "loss: 1.792459  [19200/60000]\n",
            "loss: 1.678115  [25600/60000]\n",
            "loss: 1.640383  [32000/60000]\n",
            "loss: 1.658336  [38400/60000]\n",
            "loss: 1.551344  [44800/60000]\n",
            "loss: 1.595065  [51200/60000]\n",
            "loss: 1.478713  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.9%, Avg loss: 1.499687 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.578155  [    0/60000]\n",
            "loss: 1.533143  [ 6400/60000]\n",
            "loss: 1.385327  [12800/60000]\n",
            "loss: 1.454356  [19200/60000]\n",
            "loss: 1.340039  [25600/60000]\n",
            "loss: 1.341394  [32000/60000]\n",
            "loss: 1.347566  [38400/60000]\n",
            "loss: 1.265767  [44800/60000]\n",
            "loss: 1.311801  [51200/60000]\n",
            "loss: 1.206777  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 1.237644 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.321267  [    0/60000]\n",
            "loss: 1.296052  [ 6400/60000]\n",
            "loss: 1.134134  [12800/60000]\n",
            "loss: 1.234225  [19200/60000]\n",
            "loss: 1.121591  [25600/60000]\n",
            "loss: 1.148014  [32000/60000]\n",
            "loss: 1.159718  [38400/60000]\n",
            "loss: 1.091220  [44800/60000]\n",
            "loss: 1.136470  [51200/60000]\n",
            "loss: 1.050882  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 1.076856 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#Train and test the model\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we will apply xavier initialization to the model\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.257107  [    0/60000]\n",
            "loss: 2.124851  [ 6400/60000]\n",
            "loss: 1.901465  [12800/60000]\n",
            "loss: 1.862070  [19200/60000]\n",
            "loss: 1.631832  [25600/60000]\n",
            "loss: 1.554383  [32000/60000]\n",
            "loss: 1.520262  [38400/60000]\n",
            "loss: 1.384557  [44800/60000]\n",
            "loss: 1.331362  [51200/60000]\n",
            "loss: 1.226367  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.4%, Avg loss: 1.234426 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.280512  [    0/60000]\n",
            "loss: 1.270245  [ 6400/60000]\n",
            "loss: 1.057106  [12800/60000]\n",
            "loss: 1.195777  [19200/60000]\n",
            "loss: 1.013130  [25600/60000]\n",
            "loss: 1.033009  [32000/60000]\n",
            "loss: 1.040416  [38400/60000]\n",
            "loss: 0.980558  [44800/60000]\n",
            "loss: 0.982676  [51200/60000]\n",
            "loss: 0.921903  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 0.929766 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.955894  [    0/60000]\n",
            "loss: 1.004728  [ 6400/60000]\n",
            "loss: 0.781945  [12800/60000]\n",
            "loss: 0.986839  [19200/60000]\n",
            "loss: 0.834585  [25600/60000]\n",
            "loss: 0.857618  [32000/60000]\n",
            "loss: 0.878864  [38400/60000]\n",
            "loss: 0.842356  [44800/60000]\n",
            "loss: 0.858418  [51200/60000]\n",
            "loss: 0.812345  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 0.809794 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.808654  [    0/60000]\n",
            "loss: 0.887791  [ 6400/60000]\n",
            "loss: 0.659071  [12800/60000]\n",
            "loss: 0.890902  [19200/60000]\n",
            "loss: 0.757429  [25600/60000]\n",
            "loss: 0.769506  [32000/60000]\n",
            "loss: 0.793779  [38400/60000]\n",
            "loss: 0.774039  [44800/60000]\n",
            "loss: 0.795674  [51200/60000]\n",
            "loss: 0.750337  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.3%, Avg loss: 0.742330 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.721400  [    0/60000]\n",
            "loss: 0.813657  [ 6400/60000]\n",
            "loss: 0.586894  [12800/60000]\n",
            "loss: 0.832205  [19200/60000]\n",
            "loss: 0.711421  [25600/60000]\n",
            "loss: 0.713569  [32000/60000]\n",
            "loss: 0.737290  [38400/60000]\n",
            "loss: 0.734020  [44800/60000]\n",
            "loss: 0.757156  [51200/60000]\n",
            "loss: 0.707207  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.696605 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Lets apply the xavier initialization to the model\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Now we will train and test the model again with the xavier initialization\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
